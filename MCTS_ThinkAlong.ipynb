{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from itertools import product\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchNode():\n",
    "    # class wide lookup table (state:MCTS_Node)\n",
    "    state_tbl = {}\n",
    "    # class wide state sizes\n",
    "    n_rows = 3\n",
    "    n_cols = 3\n",
    "    # class wide set to avoid double rewarding nodes in backprop\n",
    "    marked = set([])\n",
    "\n",
    "    def __init__(self,c_param=np.sqrt(2), win_reward_scalar=1, state = ((0,0,0),(0,0,0),(0,0,0)), parent=None, player = -1, reset_state_tbl=False):\n",
    "        self.state = state\n",
    "        if reset_state_tbl:\n",
    "            type(self).state_tbl = {}\n",
    "        # results[0] -> ties, results[1] -> wins, results[-1] -> losses\n",
    "        self.results = [0,0,0]\n",
    "\n",
    "        self.parents = [parent] if parent else []\n",
    "        self.children = []\n",
    "        self.player = player\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.c_param = c_param\n",
    "        self.win_reward_scalar = win_reward_scalar\n",
    "        self.tie_reward_scalar = 1 - self.win_reward_scalar\n",
    "\n",
    "        self.n_visits = 0\n",
    "\n",
    "        self.is_game_over_node = self.get_winner_code() is not None\n",
    "        self.unexplored_actions = self.get_legal_actions()\n",
    "\n",
    "    def get_legal_actions(self, state=None):\n",
    "        if not state:\n",
    "            state = self.state\n",
    "        return [(row, col) \\\n",
    "                for row, col in product(range(type(self).n_rows),range(type(self).n_cols)) \\\n",
    "                if state[row][col] == 0]\n",
    "\n",
    "    def expand(self):\n",
    "        # get an unexplored action\n",
    "        action = self.unexplored_actions.pop(0)\n",
    "        # apply that action to create the new state (make state list to mutate then tuple for tbl key)\n",
    "        new_state = self.apply_action(action=action)\n",
    "\n",
    "        if new_state in type(self).state_tbl:\n",
    "            child = type(self).state_tbl[new_state]\n",
    "            child.parents.append(self)\n",
    "        else:\n",
    "            child = MonteCarloTreeSearchNode(c_param=self.c_param, \n",
    "                                             win_reward_scalar = self.win_reward_scalar, \n",
    "                                             state=new_state, \n",
    "                                             parent=self, \n",
    "                                             player=-self.player)\n",
    "            type(self).state_tbl[new_state] = child\n",
    "        self.children.append(child)\n",
    "        return type(self).state_tbl[new_state]\n",
    "\n",
    "    def q(self):\n",
    "        wins = self.results[1]\n",
    "        ties = self.results[0]\n",
    "        return self.win_reward_scalar * wins + self.tie_reward_scalar * ties\n",
    "\n",
    "    def explore_exploit_val(self, parent):\n",
    "        return (self.q() / self.n_visits) + self.c_param * np.sqrt( np.log(parent.n_visits)/self.n_visits )\n",
    "    \n",
    "    def explore_exploit_val_list(self, parent):\n",
    "        return [child.explore_exploit_val(parent) for child in self.children]\n",
    "\n",
    "    def expore_exploit(self):\n",
    "        choices_weights = self.explore_exploit_val_list(parent=self)\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def best_child(self):\n",
    "        child_visits = [child.n_visits for child in self.children]\n",
    "        return self.children[np.argmax(child_visits)]\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.unexplored_actions) == 0\n",
    "    \n",
    "    def get_winner_code(self, state=None):\n",
    "        if not state:\n",
    "            state = self.state\n",
    "        game_result = None\n",
    "        # check rows\n",
    "        for row in range(type(self).n_rows):\n",
    "            if state[row][0] == state[row][1] == state[row][2] != 0: \n",
    "                game_result = state[row][0]\n",
    "\n",
    "        # check cols\n",
    "        for col in range(type(self).n_cols):\n",
    "            if state[0][col] == state[1][col] == state[2][col] != 0: \n",
    "                game_result = state[row][0]\n",
    "        \n",
    "        # check diagonals\n",
    "        if  (state[0][0] == state[1][1] == state[2][2] != 0) or \\\n",
    "            (state[0][2] == state[1][1] == state[2][0] != 0):\n",
    "            game_result = state[1][1] \n",
    "        \n",
    "        # check if board is full\n",
    "        elif len(self.get_legal_actions(state)) == 0:\n",
    "            game_result = 0\n",
    "        # return None => game is not over\n",
    "        return game_result\n",
    "    \n",
    "    def tree_policy(self):\n",
    "        if self.is_game_over_node: return self\n",
    "        if self.is_fully_expanded():\n",
    "            child = self.expore_exploit()\n",
    "            return child.tree_policy()\n",
    "        return self.expand()\n",
    "\n",
    "    def apply_action(self, action, state=None):\n",
    "        if not state:\n",
    "            state = self.state\n",
    "        new_state = list(list([col for col in row]) for row in state)\n",
    "        new_state[action[0]][action[1]] = -self.player\n",
    "        new_state = tuple(tuple(col for col in row) for row in new_state)\n",
    "        return new_state\n",
    "\n",
    "    def random_rollout(self, state=None):\n",
    "        if not state:\n",
    "            state = self.state\n",
    "\n",
    "        winner_code = self.get_winner_code(state)\n",
    "        if winner_code is not None:\n",
    "            return winner_code\n",
    "\n",
    "        legal_actions = self.get_legal_actions(state)\n",
    "        random_action = legal_actions[np.random.randint(len(legal_actions))]\n",
    "        new_state = self.apply_action(action = random_action, state=state)\n",
    "        # print(self.get_winner_code(state), state, legal_actions, random_action, new_state, self.get_winner_code(new_state))\n",
    "        return self.random_rollout(new_state)\n",
    "    \n",
    "    def backprop(self, reward):\n",
    "        self.results[1]  += (reward == self.player)\n",
    "        self.results[0]  += (reward == 0)\n",
    "        self.results[-1] += (reward != self.player)\n",
    "        self.n_visits += 1\n",
    "        type(self).marked.add(id(self))\n",
    "        if not self.parents: return\n",
    "        for parent in self.parents:\n",
    "            if id(parent) in type(self).marked: continue\n",
    "            parent.backprop(reward)\n",
    "\n",
    "        \n",
    "    def train(self, n_rollouts, rollouts_sofar=0, batch_size=10000):\n",
    "        # variables for runtime analysis\n",
    "        total_rollouts = n_rollouts + rollouts_sofar\n",
    "        batches_remaining = n_rollouts/batch_size\n",
    "        batch_time = 0\n",
    "        start = start_total = time.perf_counter()\n",
    "\n",
    "        for _ in range(n_rollouts):\n",
    "            # ----time prints calls----\n",
    "            rollouts_sofar += 1\n",
    "            if (rollouts_sofar+1) % batch_size == 0:\n",
    "                prev_batch_time = batch_time\n",
    "                batch_time = (time.perf_counter()-start)/60 \n",
    "                start = time.perf_counter()\n",
    "                batches_remaining -= 1\n",
    "                time_remaining = batch_time * batches_remaining\n",
    "                batch_time_diff = batch_time - prev_batch_time\n",
    "                print(f\"rollouts_ran: {rollouts_sofar + 1}/{total_rollouts}, estimated_time_remaining: {time_remaining:.2f} min, batch_time_diff: {(batch_time_diff * 60):.2f} sec             \", end='\\r')\n",
    "            # -------------------------\n",
    "\n",
    "            leaf = self.tree_policy()\n",
    "            winner_code = leaf.random_rollout()\n",
    "            type(self).marked = set([])\n",
    "            leaf.backprop(winner_code)\n",
    "        print(\"\\n*DONE* total training time: \", (time.perf_counter() - start_total)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollouts_ran: 10000/10000, estimated_time_remaining: 0.00 min, batch_time_diff: 1.71 sec             \n",
      "*DONE* total training time:  0.028460616666666282 min\n",
      "(0, 0, 0)\n",
      "(0, 0, 0)\n",
      "(-1, 0, 1)\n",
      "rollouts_ran: 10000/10000, estimated_time_remaining: 0.00 min, batch_time_diff: 1.47 sec             \n",
      "*DONE* total training time:  0.02456400333333022 min\n",
      "(-1, 0, 1)\n",
      "(0, 0, 0)\n",
      "(-1, 0, 1)\n",
      "rollouts_ran: 10000/10000, estimated_time_remaining: 0.00 min, batch_time_diff: 0.53 sec             \n",
      "*DONE* total training time:  0.008765358333334916 min\n",
      "(1, 0, 1)\n",
      "(0, -1, 0)\n",
      "(-1, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "root = MonteCarloTreeSearchNode(c_param=1,win_reward_scalar=.75,reset_state_tbl=True)\n",
    "while not root.is_game_over_node:\n",
    "    action = eval(input('action'))\n",
    "    new_state = root.apply_action(action)\n",
    "    if new_state in root.state_tbl:\n",
    "        root = root.state_tbl[new_state]\n",
    "    else:\n",
    "        root = MonteCarloTreeSearchNode(c_param=1,win_reward_scalar=.75, state=new_state, reset_state_tbl=True, player=1)\n",
    "    if root.is_game_over_node:\n",
    "        break\n",
    "    root.train(10000)\n",
    "    root = root.best_child()\n",
    "    for i in range(3):\n",
    "        print(root.state[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b308f7c9fe9e9086965beb6dcb99c12ed5dceac295ff1a757b28faad7263d027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
